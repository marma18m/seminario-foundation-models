{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de Embeddings y Clasificación con k-NN usando FAISS y CLIP\n",
    "\n",
    "En este notebook, vamos a realizar la extracción de embeddings de imágenes utilizando el modelo CLIP y almacenarlos en un índice FAISS para hacer búsquedas rápidas de vecinos más cercanos. Finalmente, implementamos un clasificador k-NN para predecir la clase de una imagen utilizando los vecinos más cercanos.\n",
    "## Contenido:\n",
    "- Preprocesamiento y carga de imágenes\n",
    "- Extracción de embeddings con CLIP\n",
    "- Almacenamiento de embeddings en FAISS\n",
    "- Clasificación k-NN basada en los embeddings\n",
    "- Ejemplo de uso del clasificador k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerías necesarias\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento de imágenes y modelo CLIP\n",
    "\n",
    "En esta sección, cargamos y preprocesamos las imágenes utilizando PIL y realizamos la extracción de los embeddings de las imágenes usando el modelo CLIP preentrenado. El modelo CLIP está diseñado para aprender representaciones de imágenes y textos de manera conjunta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos cambiamos a la carpeta anterior, porque luego para estar manipulando las imágenes es mucho más fácil meternos a nivel de la carpeta \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo clip https://github.com/openai/CLIP\n",
    "\n",
    "##device =\n",
    "##clip_model, preprocess ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa un indice de IndexFlatL2 de faiss sabiendo que el tamaño de los embeddings es de 512\n",
    "# https://github.com/facebookresearch/faiss/wiki/getting-started#in-python-1\n",
    "##faiss_index =\n",
    "\n",
    "# Folder containing the dataset\n",
    "dataset_folder = \"./data/\"\n",
    "\n",
    "# CSV to store mapping between embedding ID and class\n",
    "csv_file = \"./faiss/embeddings_mapping.csv\"\n",
    "csv_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construcción del índice FAISS\n",
    "\n",
    "Iremos recorriendo las carpetas una por una, obteniendo los embeddings y guardandolos en faiss.\n",
    "Como faiss no es una base de datos vectorial, hay que llevar un csv que mapea el identificador del embedding insertado en faiss con la clase a la que pertence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itera sobre las carpetas de las clases\n",
    "embedding_id = 0\n",
    "class_folders = [f for f in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, f))]\n",
    "\n",
    "# Use tqdm to show progress over class folders\n",
    "for class_folder in tqdm(class_folders, desc=\"Processing classes\"):\n",
    "    class_path = os.path.join(dataset_folder, class_folder)\n",
    "\n",
    "    img_files = os.listdir(class_path)\n",
    "\n",
    "    # Use tqdm to show progress over images in each class folder\n",
    "    for img_file in tqdm(img_files, desc=f\"Processing images in {class_folder}\", leave=False):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        try:\n",
    "            # Carga la imagen con PIL\n",
    "            ##img =\n",
    "            # Preprocesa la imagen con CLIP\n",
    "            ##img_preprocessed =\n",
    "\n",
    "            # Calcula el embedding de la imagen con CLIP\n",
    "            ##with ...\n",
    "                ##img_embedding =\n",
    "\n",
    "            # Añade el embeddings a faiss\n",
    "            ##\n",
    "\n",
    "            # Add mapping of embedding ID to class in the CSV\n",
    "            csv_data.append([embedding_id, class_folder])\n",
    "            embedding_id += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda el indice en esta ruta ./faiss/image_embeddings.index\"\n",
    "##\n",
    "\n",
    "# Write CSV file with mappings\n",
    "with open(csv_file, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['EmbeddingID', 'Class'])  # Header\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f\"Finished processing. FAISS index saved as 'image_embeddings.index' and CSV saved as '{csv_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploración de resultados\n",
    "\n",
    "Una vez los embeddings están obtenidos y guardados en FAISS, vamos a dibujarlos en un gráfico para ver qué pinta tendrían en 2 dimensiones\n",
    "Para eso primero hay que reducir de 512 a 2 dimensiones, lo haremos con tsne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera los embeddings de dentro del índice\n",
    "all_embeddings = []\n",
    "##for embedding_id in\n",
    "    ##embedding =\n",
    "    ##all_embeddings.append(embedding)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "# Reduce a dos dimensiones con TSNE: https://scikit-learn.org/0.16/modules/generated/sklearn.manifold.TSNE.html\n",
    "##tsne =\n",
    "##embeddings_2d ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamos la clase al embedding\n",
    "csv_file = \"./faiss/embeddings_mapping.csv\"\n",
    "class_labels = {}\n",
    "with open(csv_file, mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        embedding_id, class_label = row\n",
    "        class_labels[int(embedding_id)] = class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos los resultados\n",
    "df = pd.DataFrame(embeddings_2d, columns=[\"x\", \"y\"])\n",
    "df['Class'] = [class_labels[i] for i in range(len(all_embeddings))]\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"Class\", title=\"Image Embeddings\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de uso, encotramos las fotos más similares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud Coseno\n",
    "\n",
    "La **similitud coseno** es una medida de similitud entre dos vectores que mide el coseno del ángulo entre ellos. Se utiliza comúnmente en problemas de recuperación de información y procesamiento del lenguaje natural para medir cuán similares son dos vectores en un espacio de características, independientemente de su magnitud.\n",
    "\n",
    "La fórmula de la similitud coseno entre dos vectores A y B es:\n",
    "\n",
    "Similitud Coseno(A, B) = (A · B) / (||A|| * ||B||)\n",
    "\n",
    "\n",
    "Donde:\n",
    "- `A · B` es el producto punto entre los vectores A y B.\n",
    "- `||A||` y `||B||` son las normas (longitudes) de los vectores A y B.\n",
    "\n",
    "El valor de la similitud coseno varía entre -1 y 1, donde:\n",
    "- **1** indica que los vectores son idénticos en dirección (máxima similitud),\n",
    "- **0** indica que los vectores son ortogonales (no tienen similitud),\n",
    "- **-1** indica que los vectores son opuestos en dirección (máxima disimilitud).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el mapeo de todo\n",
    "id_to_class = {}\n",
    "\n",
    "with open(csv_file, mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        embedding_id = int(row[0])\n",
    "        class_name = row[1]\n",
    "        id_to_class[embedding_id] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para buscar imágenes similares\n",
    "def search_similar_images(input_image_path, k=5):\n",
    "    # Cargamos la imagen con PIL\n",
    "    ##img =\n",
    "\n",
    "    # La preprocesamos con CLIP\n",
    "    ##img_preprocessed = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Calculamos el embedding con clip\n",
    "    with...\n",
    "        ##input_embedding =\n",
    "\n",
    "    # utilizamos la busqueda por similitud de faiss\n",
    "    ##distances, indices =\n",
    "\n",
    "    # Retrieve the top-k most similar classes and distances\n",
    "    results = []\n",
    "    for i, index in enumerate(indices[0]):\n",
    "        class_name = id_to_class[index]\n",
    "        distance = distances[0][i]\n",
    "        results.append((class_name, distance))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos la función con una imagen de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = \"test_images/00000005.png\"  # Path to the input image\n",
    "top_k_results = search_similar_images(input_image_path, k=5)\n",
    "\n",
    "for i, (class_name, distance) in enumerate(top_k_results):\n",
    "    print(f\"Rank {i+1}: Class: {class_name}, Distance: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform k-NN classification\n",
    "def knn_classify(input_image_path, k=5):\n",
    "    # Añadimos el codigo de antes para tener las distancias y los indices\n",
    "\n",
    "    # Retrieve the classes of the k nearest neighbors\n",
    "    nearest_classes = [id_to_class[idx] for idx in indices[0]]\n",
    "\n",
    "    # Perform majority voting among the top-k classes\n",
    "    class_counter = Counter(nearest_classes)\n",
    "    predicted_class = class_counter.most_common(1)[0][0]  # Get the most common class\n",
    "\n",
    "    return predicted_class, nearest_classes, distances[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: pikachu\n",
      "Nearest Neighbors Classes: ['pikachu', 'pikachu', 'pikachu', 'pikachu', 'pikachu']\n",
      "Distances: [5.546635 7.808233 8.264306 8.264927 8.570351]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_image_path = \"test_images/00000005.png\"  # Path to the input image\n",
    "predicted_class, nearest_classes, distances = knn_classify(input_image_path, k=5)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Nearest Neighbors Classes: {nearest_classes}\")\n",
    "print(f\"Distances: {distances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".faiss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
